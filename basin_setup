#!/usr/bin/env python
import argparse
import requests
import zipfile
import StringIO
import os
import sys
import threading
from subprocess import Popen, check_output, PIPE
from colorama import init, Fore, Back, Style
from netCDF4 import Dataset
from datetime import datetime
import numpy as np
import pandas as pd
from shutil import copyfile
import zipfile

init()

DEBUG = True

class Messages():
    def __init__(self):
        self.context = {'warning':Fore.YELLOW,
                        'error':Fore.RED,
                        'ok': Fore.GREEN,
                        'normal': Style.NORMAL+Fore.WHITE,
                        'header': Style.BRIGHT}
    def build_msg(self,str_msg,context_str=None):
        """
        Constructs the desired strings for color and Style

        Args;
            str_msg: String the user wants to output
            context_str: type of print style and color, key associated with self.context
        Returns:
            final_msg: Str containing the desired colors and styles
        """

        if context_str == None:
            context_str = 'normal'

        if context_str in self.context.keys():
            if type(str_msg) == list:
                str_msg = ', '.join([str(s) for s in str_msg])

            final_msg = self.context[context_str]+ str_msg + Style.RESET_ALL
        else:
            raise ValueError("Not a valid context")
        return final_msg

    def _structure_msg(self, a_msg):
        if type(a_msg) == list:
            a_msg = ', '.join([str(s) for s in a_msg])

        if type(a_msg)!=str:
            a = str(a_msg)

        return a_msg

    def msg(self,str_msg,context_str=None):
        final_msg = self.build_msg(str_msg,context_str)
        print('\n'+final_msg)

    def dbg(self,str_msg,context_str=None):
        final_msg = self.build_msg('[DEBUG]: ','header')
        final_msg+=self._structure_msg(str_msg)
        final_msg = self.build_msg(final_msg,context_str)
        print('\n'+final_msg)

    def warn(self,str_msg):
        final_msg = self.build_msg('[WARNING]: ','header')
        final_msg = self.build_msg(final_msg+str_msg, 'warning')
        print('\n'+final_msg)

    def error(self,str_msg):
        final_msg = self.build_msg('[ERROR]: ','header')
        final_msg = self.build_msg(final_msg+str_msg,'error')
        print('\n'+final_msg)

    def respond(self,str_msg):
        final_msg = self.build_msg(str_msg, 'ok')
        print('\t'+final_msg)

out = Messages()

def parse_extent(fname):
    """
    Uses ogr to parse the information of some GIS file and returns a dict of the
    response of the things important to this script.

    Args:
        fname: Full path point to file containing GIS information

    Returns:
        extent: containing images extent in str type.
    """
    file_type = fname.split('.')[-1]
    if file_type == 'shp':
        basin_shp_info = check_output(['ogrinfo','-al',fname])
        parse_list = basin_shp_info.split('\n')
        # Parse extents from basin info
        for l in parse_list:
            if 'extent' in l.lower():
                k,v = l.split(':')
                parseable = ''.join( c for c in v if  c not in ' ()\n')
                parseable = parseable.replace('-',',')
                extent = [i for i in parseable.split(',')]
                out.dbg(extent)
                break
    elif file_type == 'tif':
        basin_shp_info = check_output(['gdalinfo',fname])
        parse_list = basin_shp_info.split('\n')
        extent = []
        for l in parse_list:
            if 'lower left' in l.lower() or 'upper right' in l.lower():
                 for w in l.split(' '):
                     try:
                         if len(extent) <=4:
                             parseable = ''.join( c for c in w if  c not in ' ,()\n')
                             extent.append(float(parseable))
                     except:
                        pass
    else:
        raise IOError("File type .{0} not recoginizeable for parsing extent".format(file_type))

    return extent

def download_zipped_url(url):
    """
    Downloads a url that is expected to be a zipped folder.
    """

    r = requests.get(url, stream=True)
    z = zipfile.ZipFile(StringIO.StringIO(r.content))
    z.extractall('~/Downloads')

def  convert_shapefile(fname,out_f):
    """
    Handles various shapefile scenario
    Given multuple shapefiles coming in convert and output a shapefile
    to out_f

    Args:
        fname: String file path

    """
    fname = os.path.abspath(os.path.expanduser(fname))

    if os.path.isfile(fname):
        name,ext = fname.split('.')
        out_dir = os.path.dirname(name)
        name = os.path.basename(name)

    else:
        raise IOError('File {0} does not exist')

    #Zipped KML file
    if ext == 'kmz':
        out.msg("KMZ (Compressed KML) file provided, unzipping...")
        #Copy to a zip
        zip_fname = os.path.join(out_dir,name+'.zip')
        copyfile(fname,zip_fname)

        zip_ref = zipfile.ZipFile(zip_fname, 'r')
        zip_ref.extractall(out_dir)
        zip_ref.close()
        os.remove(zip_fname)

        #Go find the .KML file
        out.msg("Searching for resulting KML...")
        for root, dirs, files in os.walk(out_dir):
            for f in files:
                if f.split('.')[-1] =='kml':
                    fname = os.path.join(root,f)
                    ext = 'kml'
                    name = os.path.basename(fname)
                    out.respond('KML found!{0}'.format(fname))
                    break

    #For the mean time we need to do this until we figure out how to over come converting from lat long to utm
    if ext !='shp':
        raise IOError("Please use shapfiles only in UTM")
        # out.msg("Converting {0} to shapefile...".format(ext))
        # a = Popen(['ogr2ogr','-overwrite','-f','ESRI shapefile',out_f,fname],stdout=PIPE, stderr=PIPE)
        # a.wait()
        # os.remove(fname)

def main():

    # Parge command line arguments
    p = argparse.ArgumentParser(description='Setup a new basin for SMRF. Creates all the required files.')

    p.add_argument('-f','--basin_shapefile', dest='basin_shapefile',required=True,
                    help="Path to shapefile that defines the basin")

    p.add_argument('-c','--cell_size', dest='cell_size',required=False, default=50,
                    help="Pixel size to use for the basin in meters")

    p.add_argument('-dm','--dem', dest='dem',required=True,
                    help="DEM file in geotiff")

    p.add_argument('-d','--download', dest='download',required=False, default='~/Downloads',
                    help="Location to check for veg data or download vegetation data")

    p.add_argument('-o','--output', dest='output',required=False, default='./output',
                    help="Location to output data")

    args = p.parse_args()

    msg ="Basin Setup Tool {0}".format("0.1.0")
    m = "="*(len(msg)+1)
    out.msg(m,'header')
    out.msg(msg,'header')
    out.msg(m,'header')

    #Vegetation Tau and K table from Link and Marks 1999
    # http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1085(199910)13:14/15%3C2439::AID-HYP866%3E3.0.CO;2-1/abstract
    tau = {'open':1.0,
           'deciduous':0.44,
           'mixed conifer and deciduous':0.30,
           'medium conifer':0.20,
           'dense conifer':0.16}
    mu = {'open':0.0,
           'deciduous':0.025,
           'mixed conifer and deciduous':0.033,
           'medium conifer':0.040,
           'dense conifer':0.074}

    # Keywords for landfire data sets
    veg_keywords = {'open':['Sparsely vegetated','graminoid'],
                    'deciduous':['Deciduous open tree canopy'],
                    'mixed conifer and deciduous':['Mixed evergreen-deciduous shrubland'],
                    'medium conifer':['Mixed evergreen-deciduous open tree canopy'],
                    'dense conifer':['Evergreen closed tree canopy','conifer']}


#==================== Check Inputs/ Setup ======================= #

    # Check and setup for an output dir, downloads dir, and temp dir
    required_dirs={'output':args.output,'downloads':args.download,'temp':os.path.join(args.output,'temp')}

    # Setup a workspace
    TEMP = required_dirs['temp']

    # Filename and paths and potential sources
    images={'dem':{'path':None,'source':None},
            'shapefile':{'path':None,'source':None},
            'mask':{'path':None,'source':None,'short_name':'mask'},
            'vegetation type':{'path':None,'source':None,'map':None,'short_name':'veg_type'},
            'vegetation height':{'path':None,'source':None,'map':None,'short_name':'veg_height'},
            'vegetation k':{'path':None,'source':None,'short_name':'veg_k'},
            'vegetation tau':{'path':None,'source':None,'short_name':'veg_tau'},
            'maxus':{'path':None,'source':None}}

    #Only accept .shp for now
    if args.basin_shapefile.split('.')[-1] != 'shp':
        images['shapefile']['path'] = os.path.abspath(os.path.expanduser(args.basin_shapefile))
        fname = (os.path.basename(images['shapefile']['path'])).split('.')[0]
        images['shapefile']['path'] = os.path.join(TEMP,fname+'.shp')
        convert_shapefile(args.basin_shapefile,images['shapefile']['path'])

    else:
        images['shapefile']['path'] = os.path.abspath(os.path.expanduser(args.basin_shapefile))

    # Populate Images for non downloaded files
    images['dem']['path'] = os.path.abspath(os.path.expanduser(args.dem))

    # Populate images for downloaded sources
    images['vegetation type']['source'] = 'https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_140_mosaic-US_140EVT_04252017.zip&TYPE=landfire'
    images['vegetation height']['source'] = 'https://www.landfire.gov/bulk/downloadfile.php?FNAME=US_140_mosaic-US_140EVH_12052016.zip&TYPE=landfire'
    images['vegetation type']['path'] = os.path.join(required_dirs['downloads'],'US_140EVT_04252017','Grid','us_140evt','hdr.adf')
    images['vegetation height']['path'] = os.path.join(required_dirs['downloads'],'US_140EVH_12052016','Grid','us_140evh', 'hdr.adf')

    #Make directories and create setup
    for k,d in required_dirs.items():
        full = os.path.abspath(os.path.expanduser(d))

        if not os.path.isdir(os.path.dirname(full)):
            raise IOError("Path to vegetation data/download directory does not exist.\n %s" % full)

        if k != 'downloads':
            if os.path.isdir(full):
                out.warn("{0} folder exists, potential to overwrite non-downloaded files!".format(k))

            else:
                out.msg("Making folder...")
                os.mkdir(os.path.abspath(full))
        else:
            if os.path.isdir(full):
                out.respond("{0} folder found!".format(k))
            else:
                out.msg("Making {0} folder...".format(k))
                os.mkdir(full)

    #==================== Downloads and Checking ======================= #
    for image_name in ['vegetation type', 'vegetation height']:
        info = images[image_name]
        info['path'] = os.path.abspath(os.path.expanduser(info['path']))
        images[image_name] = info

        out.msg("Checking for {0} data in {1}...".format(image_name, required_dirs['downloads']))

        #Cycle through all the downloads
        out.msg("Looking for: \n%s " % info['path'])
        if not os.path.isdir(os.path.dirname(info['path'])):

            # Missing downloaded data
            out.msg("Unzipped folder not found, check for zipped folder.")
            zipped = (os.path.dirname(os.path.dirname(info['path']))+'.zip')
            out.msg("Looking for:\n %s" % zipped)

            if not os.path.isfile(zipped):
                # Zip file does not exist
                out.warn("No data found!\nDownloading %s ..." % image_name)
                out.warn("This could take up to 20mins, so sit back and relax.")
                download_zipped_url(info['source'])

            # Downloaded but not unzipped
            else:
                out.respond("Zipped data found, unzipping...")
                z = zipfile.ZipFile(zipped)
                z.extractall(zipped)
            # Download found as expected
        else:
            out.respond("found!")

        # CSV map should be in the downloaded folder name
        map_src = os.path.dirname(os.path.dirname(os.path.dirname(info['path'])))

        out.msg("Searching for {0} map...".format(image_name))

        for root, dirnames, filenames in os.walk(map_src):
            for f in filenames:
                # Looking for the only csv
                if f.split('.')[-1] == 'csv':
                    info['map'] = os.path.join(root,f)
                    out.respond('{0} map found!\n\t{1}'.format(image_name,info['map']))
                    break

        if info['map'] == None:
            out.error("Could not find {0} map!".format(image_name))
            sys.exit()


    #==================== Processing ======================= #

    # General info for basin shape file
    out.msg("Retrieving basin outline info...\n")
    extent = parse_extent(images['shapefile']['path'])

    # Parse the desired projection
    proj = images['shapefile']['path'].split('.')[0]+'.prj'

    #Create a tiff of the mask from shapefile then conver to netcdf
    images['mask']['path'] = os.path.join(TEMP,'mask.tiff')
    z = Popen(['gdal_rasterize', '-tr',str(args.cell_size), str(args.cell_size),
               '-te', extent[0],extent[1],extent[2],extent[3],
               '-burn','1', '-ot','int', images['shapefile']['path'],
                images['mask']['path']],stdout=PIPE)
    z.wait()

    NC = os.path.abspath(os.path.join(TEMP,'clipped_mask.nc'))
    s = Popen(['gdal_translate','-of', 'NETCDF', '-sds', images['mask']['path'],NC],stdout=PIPE)
    s.wait()
    images['mask']['path'] = NC

    # Reproject and clip according to the basin mask
    for name in ['vegetation height','vegetation type', 'dem']:
        img = images[name]

        #Get data loaded in
        out.msg("Getting {0} image info...".format(name))

        img_info = check_output(['gdalinfo',img['path']])

        fname = name.replace(' ', '_')

        CLIPPED = os.path.abspath(os.path.join(TEMP,'clipped_{0}.tif'.format(fname)))

        out.msg("Reprojecting and clipping {0} rasters...".format(name))
        p = Popen(['gdalwarp','-t_srs', proj,'-te', extent[0],extent[1],
                    extent[2],extent[3], '-tr',str(args.cell_size),
                    str(args.cell_size),'-overwrite', img['path'], CLIPPED],
                    stdout=PIPE)

        p.wait()

        # Convert to Netcdf
        out.msg("Converting {0} to NetCDF...".format(name))
        NC = CLIPPED.split('.')[0]+'.nc'
        s = Popen(['gdal_translate', '-of', 'NETCDF', '-sds', CLIPPED,NC],stdout=PIPE)
        images[name]['path'] = NC
        s.wait()

    #Building the final netcdf
    out.msg("\nCreating final output for netcdf...")

    #Setup the extent using the files we just generated. The extent is decided oddly
    #In gdal so it was tough to generate the extents manually, this way we let
    # gdal managae it.
    extent = parse_extent(CLIPPED)
    x = np.arange(int(extent[0]),int(extent[2]),int(args.cell_size))
    y = np.arange(int(extent[1]),int(extent[3]),int(args.cell_size))


    out.dbg('Output netCDF file is {0}X{1}'.format(x.shape[0],y.shape[0]))
    TOPO_PATH = os.path.join(required_dirs['output'],'topo.nc')
    topo = Dataset(TOPO_PATH, 'w', format='NETCDF4', clobber=True)

    #Add modification note
    h = '[{}] Data added or updated'.format(
        datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    setattr(topo, 'last_modified', h)

    #Add dimensions, variables and such
    topo.createDimension('y', y.shape[0])
    topo.createDimension('x', x.shape[0])

    topo.createVariable('y', 'f', 'y')
    topo.createVariable('x', 'f', 'x')

    # the y variable attributes
    topo.variables['y'].setncattr(
            'units',
            'meters')
    topo.variables['y'].setncattr(
            'description',
            'UTM, north south')
    topo.variables['y'].setncattr(
            'long_name',
            'y coordinate')

    # the x variable attributes
    topo.variables['x'].setncattr(
            'units',
            'meters')
    topo.variables['x'].setncattr(
            'description',
            'UTM, east west')
    topo.variables['x'].setncattr(
            'long_name',
            'x coordinate')

    #Cycle through and add/generate all the images that go into the topo.nc
    for name, image in images.items():
        if name not in ['shapefile','maxus']:
            out.msg("Adding variable {0}".format(name))

            if 'short_name' in image.keys():
                short_name = image['short_name']
            else:
                short_name = name

            topo.createVariable(short_name, 'f',('y','x'))
            topo.variables[short_name].setncattr('long_name',name)

            if image['path'] != None and os.path.isfile(image['path']):
                d = Dataset(image['path'],'r')
                try:
                    topo.variables[short_name][:] = d.variables['Band1'][:]
                except Exception as e:
                    print short_name
                    #print(d.variables)
                    raise(e)
                d.close()


    #==================== Post-Processing ======================= #
    #CALCULATE TAU AND K
    out.msg('Calculating veg_tau and veg_k...')

    #Open veg tyep data set convert to an array and creat empty vars to populate
    d = Dataset(images['vegetation type']['path'],'r')
    veggies = np.array(d.variables['Band1'][:])
    veg_tau = np.zeros(veggies.shape)
    veg_k = np.zeros(veggies.shape)

    #Get the unique values available in the array
    veg_values = np.unique(np.array(topo.variables['veg_type'][:]))
    d.close()

    #Open the key provided by Landfire to auto assign values in Tau and K
    f = images['vegetation type']['map']
    veg_map = pd.read_csv(f)
    veg_map = veg_map.ix[veg_map['VALUE'].isin(veg_values)]

    #Cycle through the key words in the table used for conversion and assign values
    for k,keywords in veg_keywords.items():
        for word in keywords:
            veg_filter = veg_map['VALUE'].ix[veg_map['EVT_SBCLS']==word]
            for vtype in veg_filter.tolist():
                veg_tau[veggies==vtype]=tau[k]
                veg_k[veggies==vtype]=mu[k]

    topo.variables['veg_tau'][:] = veg_tau
    topo.variables['veg_k'][:] = veg_k

    #REMAP VEGETATION HEIGHT
    out.msg('Estimating vegetation heights...')

    #Open veg heights and convert to array
    d = Dataset(images['vegetation height']['path'],'r')
    veg_height = np.array(d.variables['Band1'][:])
    final_veg_height = np.zeros(veg_height.shape)

    height_values = np.unique(np.array(topo.variables['veg_height'][:]))
    d.close()

    #Open the key provided by Landfire to auto assign values in the final veg height
    f = images['vegetation height']['map']
    height_key = pd.read_csv(f)
    height_key = height_key.ix[height_key['VALUE'].isin(height_values)]
    height_map = {}
    #print(height_key)

    #Parse the veg height ranges and create a dictionary for converting
    for i,row in height_key.iterrows():
        description = row['CLASSNAMES']
        height_range = []
        for word in description.split(' '):
            try:
                height_range.append(float(word))
            except:
                #Exceptions to veg heights not provided with a height that are guessed
                if description in ['Sparse Vegetation Height','NASS-Row Crop',' NASS-Close Grown Crop']:
                    height_range = [0,0.1]

                elif description in ['Developed-Upland Deciduous Forest','Developed-Upland Mixed Forest','Developed - Medium Intensity']:
                    height_range = [0,5]
                elif description in ['Developed-Upland Shrubland','NASS-Vineyard']:
                    height_range = [0,2]
                elif description in ['Developed-Upland Herbaceous','NASS-Wheat']:
                    height_range = [0,1]
        height_map[row['VALUE']] = height_range

    count = 0

    #Reassign random height values or max values
    for k,height in height_map.items():
            ind = veg_height==k
            #Randomized
            # rand = np.random.rand(*veg_height[ind].shape)
            # final_veg_height[ind] = (rand* height[1]) + height[0]

            #Max
            #final_veg_height[ind] = max(height)
            #count += len(ind)

            #Avg
            if len(height) == 2:
                final_veg_height[ind] = np.mean(height)
                count += len(ind)
    #Reassign to the NetCDF and finish
    topo.variables['veg_height'][:] = final_veg_height
    topo.close()

if __name__ == '__main__':
    main()
